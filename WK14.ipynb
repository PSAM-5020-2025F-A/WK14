{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14\n",
    "\n",
    "Deep Learning Models and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/nn_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/bob-ross.tar.gz | tar xz\n",
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz\n",
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/metfaces.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argsort, asarray\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "from torch import nn, Tensor, no_grad, cuda\n",
    "from torch import float32 as t_float32, uint8 as t_uint8\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from nn_utils import get_num_params\n",
    "\n",
    "DEVICE = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "### CNNs:\n",
    "\n",
    "These are the networks that use convolution kernels to extract location-independent features from images.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_layers.jpg\" height=\"320px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/rpdq7DSd/cnn-layers.jpg\" height=\"320px\"/>\n",
    "\n",
    "### ResNet:\n",
    "\n",
    "[ResNet](https://arxiv.org/abs/1512.03385) is a specific CNN architecture. It combines convolution, pooling and residual layers to enable deep networks for visual tasks.\n",
    "\n",
    "The [pre-trained ResNet](https://pytorch.org/hub/pytorch_vision_resnet/) models in the `PyTorch` library were trained on the [ImageNet](https://image-net.org/download.php) dataset, which contains $1\\text{,}281\\text{,}167$ training images classified into $1\\text{,}000$ classes.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_01.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hP20Rn9D/resnet34-01.jpg\" width=\"900px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### ResNet without a head:\n",
    "\n",
    "Due to the diversity of the millions of images used in training, these pre-trained `ResNet` models are capable of detecting very specific patterns and features. The final layers of the network, right before the final, fully-connected, classification layer, contain very dense representations of the content and style of the images being passed through the network.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_embed.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/W1v1shXh/resnet-embed.jpg\" width=\"900px\" />\n",
    "\n",
    "We can instantiate a `ResNet`, remove its classification layer and use the outputs of its final neurons as encoded features. This is what is referred to as _embeddings_: the high dimensional information of an image gets _embedded_ into a lower-dimensional representation. Instead of $500 \\times 500$ pixels of color information, we get $2048$ features of visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a model\n",
    "\n",
    "We'll use the pre-trained `ResNet50` model from the `PyTorch` library and replace its final, fully-connected, layer (`model.fc`) with an `Identity` layer, which doesn't do anything, just passes whatever it gets as input to its output.\n",
    "\n",
    "The `model.eval()` function tells `PyTorch` that we're not training any models, just using them, so some parts of the network, like `Dropout` layers can be simplified/disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT).to(DEVICE)\n",
    "model.fc = nn.Identity()\n",
    "model.eval()\n",
    "\n",
    "get_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ResNet` pre-processing\n",
    "\n",
    "Like in `WK13`, we have to pre-process our data so it looks like the data that was used during training.\n",
    "\n",
    "This is as if someone ran `fit()` on the training data, and now we have to run `transform()` to get the data into the same units, size and shape as the training data. But there's no pre-fabricated `transform()` function so we have to put together a pre-processing routine using the `Compose` functionality of `PyTorch`.\n",
    "\n",
    "From https://pytorch.org/hub/pytorch_vision_resnet/:\n",
    "\n",
    "_All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]._\n",
    "\n",
    "The following `PyTorch` transformations will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_transforms = v2.Compose([\n",
    "  v2.ToDtype(t_uint8),\n",
    "  v2.Resize(224),\n",
    "  v2.ToDtype(t_float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Let's load the data from `./data/image/bob-ross/`. This dataset contains $400$ images of most of the paintings done by Bob Ross in his TV show from $1983$ to $1994$.\n",
    "\n",
    "The images all have the same dimensions, so we could load them into a `DataFrame` and then do the processing, but it is a little easier to just use a for loop to iterate over all of the files and:\n",
    "- Open each image as a `PIL` image\n",
    "- Put pixels into a `Tensor` object\n",
    "- Pre-process the image using the transformations defined above (size, normalize, etc)\n",
    "- Pass the pre-processed image through the model and save the resulting embeddings as a normal `Python` list\n",
    "\n",
    "Some observations about the code:\n",
    "- `asarray()`: shortcut to extract pixels from a `PIL` image and keep them in $2D$ (width x height)\n",
    "- `permute()`: re-orders the color channels into $3$ single-channel layers instead of $1$ RGB layer; required by CNNs\n",
    "- `out[0]`: `PyTorch` models are designed to analyze multiple inputs at once, `[0]` is the location of our result\n",
    "- `tolist()`: moves the resulting embedding from the GPU to the CPU and converts it to a regular `Python` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./data/image/bob-ross\"\n",
    "fnames = sorted([f for f in listdir(IMG_DIR) if f.endswith(\"jpg\")])\n",
    "\n",
    "img_embeddings = []\n",
    "\n",
    "for f in fnames:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{f}\")\n",
    "  img_t = Tensor(asarray([img])).permute(0,3,1,2)  # 1 x c x h x w\n",
    "  img_t = res_transforms(img_t).to(DEVICE)\n",
    "  out = model(img_t)\n",
    "  img_embeddings.append(out[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_embeddings))\n",
    "print(len(img_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "Ok. Now we have $2048$ features for each of our images.\n",
    "\n",
    "Since these features encode dense visual information about the images, we can use them to navigate our data in different ways.\n",
    "\n",
    "For example, we can use clustering to explore image groups, or use specific image embeddings to find similar images.\n",
    "\n",
    "The process for the latter involves using:\n",
    "- `euclidean_distances()` to compute embedding distances between every possible pair of images.\n",
    "- `argsort()` to order the images by their distances to a reference image\n",
    "\n",
    "For example, let's say a dataset of $4$ images has the following $3$-dimensional embeddings:\n",
    "\n",
    "||embedding &nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "|-|-|\n",
    "|img0|$\\left[2.0, 5.0, 3.0\\right]$|\n",
    "|img1|$\\left[1.0, 4.0, 6.0\\right]$|\n",
    "|img2|$\\left[8.0, 2.0, 1.0\\right]$|\n",
    "|img3|$\\left[1.0, 6.0, 1.0\\right]$|\n",
    "\n",
    "Using `euclidean_distances()` to compute the pairwise distances between image embeddings gives:\n",
    "\n",
    "||img0|img1|img2|img3|\n",
    "|-|-|-|-|-|\n",
    "|**img0**|$0.00$|$3.32$|$7.00$|$2.45$|\n",
    "|**img1**|$3.32$|$0.00$|$8.83$|$5.38$|\n",
    "|**img2**|$7.00$|$8.83$|$0.00$|$8.06$|\n",
    "|**img3**|$2.45$|$5.38$|$8.06$|$0.00$|\n",
    "\n",
    "And `argsort()` gives us the indexes of the columns that would sort each row by distance values:\n",
    "\n",
    "|img0|$0$|$3$|$1$|$2$|\n",
    "|-|-|-|-|-|\n",
    "|**img1**|$1$|$0$|$3$|$2$|\n",
    "|**img2**|$2$|$0$|$3$|$1$|\n",
    "|**img3**|$3$|$0$|$1$|$2$|\n",
    "\n",
    "The `bob-ross` dataset has $400$ images and we're using $2048$-dimensional embeddings, but the idea is the same.\n",
    "\n",
    "If we want to see what images are the most similar to image $10$, we can look in row $10$ of the sorted indexes: the very first value there will be the index of image $10$ itself, and the other indexes are sorted by how similar their corresponding image is to image $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_idx = 10\n",
    "dists = euclidean_distances(img_embeddings)\n",
    "sorted_idxs = argsort(dists[src_idx])\n",
    "\n",
    "for idx in sorted_idxs[:5]:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "\n",
    "We can repeat this exercise, but use other images as queries for our search.\n",
    "\n",
    "For example, we can search for Bob Ross paintings that are most similar to landscape paintings or photographs from other artists.\n",
    "\n",
    "These are just some examples, but feel free to explore other possibilities:\n",
    "\n",
    "\n",
    "<img src=\"https://samuelearp.com/wp-content/uploads/2023/10/IMG_1512-scaled.jpeg\" height=\"200px\"><br>\n",
    "[url](https://samuelearp.com/wp-content/uploads/2023/10/IMG_1512-scaled.jpeg)\n",
    "\n",
    "<img src=\"https://i0.wp.com/inesepogagallery.com/wp-content/uploads/1-Old-grey-barn-in-meadow-framed-acrylic-painting.jpg\" height=\"200px\"><br>\n",
    "[url](https://i0.wp.com/inesepogagallery.com/wp-content/uploads/1-Old-grey-barn-in-meadow-framed-acrylic-painting.jpg)\n",
    "\n",
    "<img src=\"https://posterjack.ca/cdn/shop/articles/landscape_photography_tips_featured_image.jpg?v=1563408049&width=2048\" height=\"200px\"><br>\n",
    "[url](https://posterjack.ca/cdn/shop/articles/landscape_photography_tips_featured_image.jpg?v=1563408049&width=2048)\n",
    "\n",
    "<img src=\"https://preview.redd.it/ansel-adams-the-tetons-and-the-snake-river-1942-grand-teton-v0-jfoc6jdjzpt81.jpg?width=1080&crop=smart&auto=webp&s=f687f44a04706b0571d8b30fb1f0ef8dcf32691e\" height=\"200px\"><br>\n",
    "[url](https://preview.redd.it/ansel-adams-the-tetons-and-the-snake-river-1942-grand-teton-v0-jfoc6jdjzpt81.jpg?width=1080&crop=smart&auto=webp&s=f687f44a04706b0571d8b30fb1f0ef8dcf32691e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up and perform search using non-Bob-Ross images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "\n",
    "Embeddings are super useful and CNNs were the first networks that really allowed us to do this kind of feature extraction by leveraging learned characteristics of massive datasets. People even used CNNs on non-image data, by coming up with clever transformations to encode different types of data into image-like representations.\n",
    "\n",
    "This started to change around $2017$ when a new type of network architecture was proposed for text translation models. By $2020$, _transformer_ networks had been adapted to work with any kind of input, not just text. \n",
    "\n",
    "`CLIP` is an example of a contrastive, transformer-based, model. Contrastive here means that it was trained on multi-modal inputs (images and text) at the same time. This allows us to create text and image embeddings using the same _units_. Embedding values for the word _dog_ will be similar to embedding values for images of dogs.\n",
    "\n",
    "We can access pre-trained `CLIP` models using the `transformers` library. This is a library developed on top of `PyTorch` to facilitate the use of pre-trained transformer models.\n",
    "\n",
    "We start by creating pre-processor and model objects from the same training instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
    "DEVICE = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed\n",
    "\n",
    "This will look familiar, and perhaps even simpler, since the `CLIP` pre-processor is already built for us and we can just use it directly on `PIL` images.\n",
    "\n",
    "The `no_grad()` function below is another way to tell `PyTorch` that we're not training a network, so it can optimize some of its internal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./data/image/bob-ross\"\n",
    "fnames = sorted([f for f in listdir(IMG_DIR) if f.endswith(\"jpg\")])\n",
    "\n",
    "img = PImage.open(f\"{IMG_DIR}/{fnames[0]}\")\n",
    "\n",
    "img_t = clip_processor(images=img, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "with no_grad():\n",
    "  clip_embedding = clip_model.get_image_features(**img_t)[0]\n",
    "\n",
    "clip_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `CLIP` embeddings are $768$-dimensional.\n",
    "\n",
    "Let's compute `CLIP` embeddings for all of the images in the `bob-ross` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./data/image/bob-ross\"\n",
    "fnames = sorted([f for f in listdir(IMG_DIR) if f.endswith(\"jpg\")])\n",
    "\n",
    "img_embeddings = []\n",
    "\n",
    "for f in fnames:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{f}\")\n",
    "  img_t = clip_processor(images=img, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "  with no_grad():\n",
    "    out = clip_model.get_image_features(**img_t)\n",
    "    img_embeddings.append(out[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can use `euclidean_distances()` and `argsort()` to find similar images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_idx = 10\n",
    "dists = euclidean_distances(img_embeddings)\n",
    "sorted_idxs = argsort(dists[src_idx])\n",
    "\n",
    "for idx in sorted_idxs[:5]:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)\n",
    "\n",
    "# TODO: does cosine_distances() change results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But wait ! There's more !\n",
    "\n",
    "So... `CLIP` is $20$ times larger, takes longer to run and the embeddings are \"only\" $768$ values...\n",
    "\n",
    "What did we get?\n",
    "\n",
    "Since this is a contrastive language-image model, we can encode text and use text embeddings to navigate images.\n",
    "\n",
    "The process is similar. We pre-process our string and then get its embedding from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"barn\"\n",
    "\n",
    "txt_t = clip_processor(text=txt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with no_grad():\n",
    "  txt_embedding = clip_model.get_text_features(**txt_t)[0].tolist()\n",
    "\n",
    "len(txt_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can order our images by their distance from the text embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = euclidean_distances([txt_embedding], img_embeddings)\n",
    "sorted_idxs = argsort(dists[0])\n",
    "\n",
    "for idx in sorted_idxs[:5]:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any text\n",
    "\n",
    "Text of any length gets embedded into the same number of features, so we can even try to search using full sentences and more specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"cottage in the snow\"\n",
    "\n",
    "txt_t = clip_processor(text=txt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with no_grad():\n",
    "  txt_embedding = clip_model.get_text_features(**txt_t)[0].tolist()\n",
    "\n",
    "len(txt_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = euclidean_distances([txt_embedding], img_embeddings)\n",
    "sorted_idxs = argsort(dists[0])\n",
    "\n",
    "for idx in sorted_idxs[:5]:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding arithmetic (part 1)\n",
    "\n",
    "The above worked ok, but some of the images didn't have a cottage, or snow.\n",
    "\n",
    "This could be because the phrase \"_cottage in the snow_\" is nudging the model towards some more specific concepts and distancing it from the space of _paintings_.\n",
    "\n",
    "One way we can embed multiple terms without steering the embedding towards some more specific concepts is by adding individual embeddings.\n",
    "\n",
    "Instead of embedding \"_cottage in the snow_\", we'll embed the terms _cottage_ and _snow_ separately and add them together before searching for images.\n",
    "\n",
    "The following cell pre-processes and embeds a list of words, like we did a list of images above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [\"cottage\", \"snow\"]\n",
    "\n",
    "txt_embeddings = []\n",
    "\n",
    "for t in txt:\n",
    "  txt_t = clip_processor(text=t, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(DEVICE)\n",
    "  with no_grad():\n",
    "    out = clip_model.get_text_features(**txt_t)\n",
    "    txt_embeddings.append(out[0].tolist())\n",
    "\n",
    "print(f\"txt_embeddings shape: ({len(txt_embeddings)}, {len(txt_embeddings[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the terms from the individual embeddings, and even add a bit of extra importance to _snow_ by multiplying its embedding terms by $2$, before performing the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_embedding = [t0 + 2 * t1 for t0,t1 in zip(txt_embeddings[0], txt_embeddings[1])]\n",
    "\n",
    "dists = euclidean_distances([txt_embedding], img_embeddings)\n",
    "sorted_idxs = argsort(dists[0])\n",
    "\n",
    "for idx in sorted_idxs[:5]:\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "\n",
    "Instead of finding images that are similar to a given text, we can change our code slightly and use embeddings to determine which label from a pre-determined set of possible labels best describes a given image.\n",
    "\n",
    "This technique of leveraging generic model knowledge to create a classification model with dynamic labels is sometimes called zero-shot classification: we can get the model to classify images even though it was not trained to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [\"mountain\", \"forest\", \"lake\"]\n",
    "\n",
    "txt_t = clip_processor(text=txt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with no_grad():\n",
    "  txt_embeddings = clip_model.get_text_features(**txt_t).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx in range(5):\n",
    "  dists = euclidean_distances([img_embeddings[img_idx]], txt_embeddings)\n",
    "  sorted_idxs = argsort(dists[0])\n",
    "\n",
    "  img = PImage.open(f\"{IMG_DIR}/{fnames[img_idx]}\")\n",
    "  img.thumbnail((256, 256))\n",
    "  display(img)\n",
    "\n",
    "  print(txt[sorted_idxs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More embedding arithmetic\n",
    "\n",
    "Everything is a number, so why not ?\n",
    "\n",
    "Since `CLIP` embeddings are able to represent similar concepts using similar numbers, regardless of whether those concepts are initially expressed as text or images, subtracting the embedding for the word \"snow\" from the embedding of an image of a Bob Ross painting with snow, should leave us with an embedding of a Bob Ross painting without snow.\n",
    "\n",
    "We can't turn the resulting embedding into a brand new image, but we can use it to search existing images for the one that most closely resembles the Bob Ross painting with snow, but without the snow.\n",
    "\n",
    "Let's encode \"snow\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"snow\"\n",
    "\n",
    "txt_t = clip_processor(text=txt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with no_grad():\n",
    "  txt_embedding = clip_model.get_text_features(**txt_t)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode an image of a painting with snow.\n",
    "\n",
    "The following indexes are for paintings with snow:\n",
    "\n",
    "```python\n",
    "snow_img_idxs = [201, 254, 61, 223, 16, 184, 89, 0, 124, 111]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 201\n",
    "\n",
    "snow_img = PImage.open(f\"{IMG_DIR}/{fnames[img_idx]}\")\n",
    "snow_img.thumbnail((256, 256))\n",
    "display(snow_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subtract the \"snow\" embedding from the embedding of the selected image.\n",
    "\n",
    "We'll use a comprehension and `zip()` to do the subtraction of each individual item.\n",
    "\n",
    "then, we use `euclidean_distances()` and `argsort()` to find the image that most resembles the resulting embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_snow_embedding = [i - t for i,t in zip(img_embeddings[img_idx], txt_embedding)]\n",
    "\n",
    "dists = euclidean_distances([no_snow_embedding], img_embeddings)\n",
    "sorted_idxs = argsort(dists[0])\n",
    "\n",
    "no_snow_img = PImage.open(f\"{IMG_DIR}/{fnames[sorted_idxs[0]]}\")\n",
    "no_snow_img.thumbnail((256, 256))\n",
    "\n",
    "# combine images horizontally\n",
    "img = PImage.new(\"RGB\", (2 * snow_img.width, snow_img.height))\n",
    "img.paste(snow_img, (0,0))\n",
    "img.paste(no_snow_img, (snow_img.width, 0))\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for a lookalike\n",
    "\n",
    "Use the `metfaces` dataset to look for faces most-similar to Bob Ross:\n",
    "\n",
    "<img src=\"https://www.bobross.com/content/bob_ross_img.png\" height=\"300px\">\n",
    "\n",
    "[url](https://www.bobross.com/content/bob_ross_img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read and embed all metfaces images, and then search for Bob Ross lookalikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SigLIP\n",
    "\n",
    "This is a newer contrastive model.\n",
    "\n",
    "We can load its pre-processor and pre-trained weights using the `transformers` library and its interface will be the same as the interface for the `CLIP` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "SIGLIP_MODEL_NAME = \"google/siglip2-giant-opt-patch16-256\"\n",
    "\n",
    "siglip_processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_NAME)\n",
    "siglip_model = AutoModel.from_pretrained(SIGLIP_MODEL_NAME, device_map=\"auto\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try SigLIP (on a different dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
